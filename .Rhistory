ggplot(data, aes(x = x, y = y, color = cluster)) +
geom_point() +
geom_smooth(method = "lm", se = FALSE) +
ggtitle("Relationships within Clusters")
# Generate data for cluster 2
x2 <- rnorm(n_per_cluster, mean = 3, sd = 0.5)
y2 <- -2 * x2 + rnorm(n_per_cluster, mean = 4, sd = 0.5) # Negative trend within cluster
# Generate data for cluster 3
x3 <- rnorm(n_per_cluster, mean = 10, sd = 0.5)
y3 <- 0.5 * x3 + rnorm(n_per_cluster, mean = 2, sd = 0.5) # Weak positive trend within cluster
# Combine clusters into a single dataset
x <- c(x1, x2, x3)
y <- c(y1, y2, y3)
cluster <- factor(rep(1:3, each = n_per_cluster))
data <- data.frame(x = x, y = y, cluster = cluster)
# Plot data by clusters
ggplot(data, aes(x = x, y = y, color = cluster)) +
geom_point() +
geom_smooth(method = "lm", se = FALSE) +
ggtitle("Relationships within Clusters")
library(ggplot2)
# Set seed for reproducibility
set.seed(123)
# Define cluster parameters
n_per_cluster <- 50
# Generate data for cluster 1
x1 <- rnorm(n_per_cluster, mean = 2, sd = 0.5)
y1 <- 1.2 * x1 + rnorm(n_per_cluster, mean = -1, sd = 0.5) # Positive trend within cluster
# Generate data for cluster 2
x2 <- rnorm(n_per_cluster, mean = 3, sd = 0.5)
y2 <- -2 * x2 + rnorm(n_per_cluster, mean = 4, sd = 0.5) # Negative trend within cluster
# Generate data for cluster 3
x3 <- rnorm(n_per_cluster, mean = 4, sd = 0.5)
y3 <- 0.5 * x3 + rnorm(n_per_cluster, mean = 2, sd = 0.5) # Weak positive trend within cluster
# Combine clusters into a single dataset
x <- c(x1, x2, x3)
y <- c(y1, y2, y3)
cluster <- factor(rep(1:3, each = n_per_cluster))
data <- data.frame(x = x, y = y, cluster = cluster)
# Plot data by clusters
ggplot(data, aes(x = x, y = y, color = cluster)) +
geom_point() +
geom_smooth(method = "lm", se = FALSE) +
ggtitle("Relationships within Clusters")
d <- data.frame(y = data$y, x = data$x)
res <- clustreg(d = d, k = 3, t = 100, q = 42)
d = read.csv('C:/Users/jeffr/Downloads/wages.csv', header=T) # assign data set to d
d <- data.frame(y = data$y, x1 = data$x, x2 = rnorm(150) )
res <- clustreg(d = d, k = 3, t = 100, q = 42)
table(res$clusters,data$cluster)
res$obs
table(res$obs,data$cluster)
# Generate data for cluster 1
x1 <- rnorm(n_per_cluster, mean = 2, sd = 0.5)
y1 <- 1.2 * x1 + rnorm(n_per_cluster, mean = -1, sd = 0.5) # Positive trend within cluster
# Generate data for cluster 2
x2 <- rnorm(n_per_cluster, mean = 3, sd = 0.5)
y2 <- -2 * x2 + rnorm(n_per_cluster, mean = 10, sd = 0.5) # Negative trend within cluster
# Generate data for cluster 3
x3 <- rnorm(n_per_cluster, mean = 4, sd = 0.5)
y3 <- 0.5 * x3 + rnorm(n_per_cluster, mean = 20, sd = 0.5) # Weak positive trend within cluster
# Combine clusters into a single dataset
x <- c(x1, x2, x3)
y <- c(y1, y2, y3)
cluster <- factor(rep(1:3, each = n_per_cluster))
data <- data.frame(x = x, y = y, cluster = cluster)
# Plot data by clusters
ggplot(data, aes(x = x, y = y, color = cluster)) +
geom_point() +
geom_smooth(method = "lm", se = FALSE) +
ggtitle("Relationships within Clusters")
# Generate data for cluster 2
x2 <- rnorm(n_per_cluster, mean = 3, sd = 0.5)
y2 <- -2 * x2 + rnorm(n_per_cluster, mean = 10, sd = 0.5) # Negative trend within cluster
# Generate data for cluster 3
x3 <- rnorm(n_per_cluster, mean = 4, sd = 0.5)
y3 <- 0.5 * x3 + rnorm(n_per_cluster, mean = 20, sd = 0.5) # Weak positive trend within cluster
# Combine clusters into a single dataset
x <- c(x1, x2, x3)
y <- c(y1, y2, y3)
cluster <- factor(rep(1:3, each = n_per_cluster))
data <- data.frame(x = x, y = y, cluster = cluster)
# Plot data by clusters
ggplot(data, aes(x = x, y = y, color = cluster)) +
geom_point() +
geom_smooth(method = "lm", se = FALSE) +
ggtitle("Relationships within Clusters")
# Generate data for cluster 2
x2 <- rnorm(n_per_cluster, mean = 10, sd = 0.5)
y2 <- -2 * x2 + rnorm(n_per_cluster, mean = 10, sd = 0.5) # Negative trend within cluster
# Generate data for cluster 3
x3 <- rnorm(n_per_cluster, mean = 4, sd = 0.5)
y3 <- 0.5 * x3 + rnorm(n_per_cluster, mean = 20, sd = 0.5) # Weak positive trend within cluster
# Combine clusters into a single dataset
x <- c(x1, x2, x3)
y <- c(y1, y2, y3)
cluster <- factor(rep(1:3, each = n_per_cluster))
data <- data.frame(x = x, y = y, cluster = cluster)
# Plot data by clusters
ggplot(data, aes(x = x, y = y, color = cluster)) +
geom_point() +
geom_smooth(method = "lm", se = FALSE) +
ggtitle("Relationships within Clusters")
# Analyze pooled data
ggplot(data, aes(x = x, y = y)) +
geom_point() +
geom_smooth(method = "lm", color = "blue", se = FALSE) +
ggtitle("Relationship in Pooled Data")
d <- data.frame(y = data$y, x1 = data$x, x2 = rnorm(150) )
res <- clustreg(d = d, k = 3, t = 100, q = 42)
table(res$obs,data$cluster)
# Analyze pooled data
ggplot(data, aes(x = x, y = y)) +
geom_point() +
geom_smooth(method = "lm", color = "blue", se = FALSE) +
ggtitle("Relationship in Pooled Data")
d <- data.frame(y = data$y, x1 = data$x, x2 = rnorm(150) )
res <- clustreg(d = d, k = 3, t = 100, q = 42)
table(res$obs,data$cluster)
res <- clustreg(d = d, k = 3, t = 100, q = 42)
table(res$obs,data$cluster)
res <- clustreg(d = d, k = 3, t = 100, q = 42)
table(res$obs,data$cluster)
res <- clustreg(d = d, k = 3, t = 100, q = 42)
table(res$obs,data$cluster)
res <- clustreg(d = d, k = 3, t = 100, q = 42)
table(res$obs,data$cluster)
res <- clustreg(d = d, k = 3, t = 100, q = 4)
table(res$obs,data$cluster)
install.packages("clustvarsel")
library(clustvarsel)
install.packages("flexmix")
library(flexmix)
?flexmix
flexmix(y ~ x1, data = d, k = 3,
control = list(verb = 5, iter = 100))
# Plot data by clusters
ggplot(data, aes(x = x, y = y, color = cluster)) +
geom_point() +
geom_smooth(method = "lm", se = FALSE) +
ggtitle("Relationships within Clusters")
d <- data.frame(y = data$y, x1 = data$x, x2 = rnorm(150) )
res <- clustreg(d = d, k = 3, t = 100, q = 4)
table(res$obs,data$cluster)
flexmix(y ~ x1, data = d, k = 3,
control = list(verb = 5, iter = 100))
test=flexmix(y ~ x1, data = d, k = 3,
control = list(verb = 5, iter = 100))
test@cluster
test=flexmix(y ~ x1, data = d, k = 4,
control = list(verb = 5, iter = 100))
test@cluster
clusterwise_regression <- function(data, k, max_iter = 100, tol = 1e-6) {
# Input:
# - data: A data frame with predictors and response (last column as response)
# - k: Number of clusters
# - max_iter: Maximum number of iterations for ALS
# - tol: Convergence tolerance
# Extract predictors (X) and response (y)
X <- as.matrix(data[, -ncol(data)])
y <- as.matrix(data[, ncol(data)])
n <- nrow(data)
# Initialize cluster assignments randomly
clusters <- sample(1:k, n, replace = TRUE)
betas <- list()
# Alternating Least Squares Loop
for (iter in 1:max_iter) {
old_clusters <- clusters
# Step 1: Update regression coefficients for each cluster using OLS
for (j in 1:k) {
if (sum(clusters == j) > 0) {
X_cluster <- X[clusters == j, , drop = FALSE]
y_cluster <- y[clusters == j, , drop = FALSE]
betas[[j]] <- solve(t(X_cluster) %*% X_cluster) %*% t(X_cluster) %*% y_cluster
} else {
betas[[j]] <- matrix(0, ncol(X), 1) # Handle empty clusters
}
}
# Step 2: Assign each observation to the closest cluster based on regression fit
distances <- matrix(0, n, k)
for (j in 1:k) {
beta_j <- betas[[j]]
if (!is.null(beta_j)) {
distances[, j] <- rowSums((y - X %*% beta_j)^2)
}
}
clusters <- max.col(-distances, ties.method = "random")
# Convergence Check
if (all(clusters == old_clusters)) break
}
# Return results
list(
clusters = clusters,
betas = betas,
iterations = iter,
converged = iter < max_iter
)
}
# Simulated data
set.seed(123)
n <- 150
x1 <- rnorm(n)
x2 <- rnorm(n)
# Cluster 1
y1 <- 3 * x1 + 2 * x2 + rnorm(n, sd = 0.5)
# Cluster 2
y2 <- -2 * x1 + rnorm(n, sd = 0.5)
# Cluster 3
y3 <- 1 * x1 - 1 * x2 + rnorm(n, sd = 0.5)
data <- data.frame(
x1 = c(x1[1:50], x1[51:100], x1[101:150]),
x2 = c(x2[1:50], x2[51:100], x2[101:150]),
y = c(y1[1:50], y2[51:100], y3[101:150])
)
# Apply clusterwise regression
result <- clusterwise_regression(data, k = 3)
# Output
print(result$clusters) # Cluster assignments
print(result$betas)    # Regression coefficients for each cluster
print(result$converged) # Convergence status
library(plotly)
plot_ly(data = data, x = ~x1, y = ~y, z = ~x2)
d <- data.frame(x1 = data$x, x2 = rnorm(150), y = data$y )
# Set seed for reproducibility
set.seed(123)
# Define cluster parameters
n_per_cluster <- 50
# Generate data for cluster 1
x1 <- rnorm(n_per_cluster, mean = 2, sd = 0.5)
y1 <- 1.2 * x1 + rnorm(n_per_cluster, mean = -1, sd = 0.5) # Positive trend within cluster
# Generate data for cluster 2
x2 <- rnorm(n_per_cluster, mean = 10, sd = 0.5)
y2 <- -2 * x2 + rnorm(n_per_cluster, mean = 10, sd = 0.5) # Negative trend within cluster
# Generate data for cluster 3
x3 <- rnorm(n_per_cluster, mean = 4, sd = 0.5)
y3 <- 0.5 * x3 + rnorm(n_per_cluster, mean = 20, sd = 0.5) # Weak positive trend within cluster
# Combine clusters into a single dataset
x <- c(x1, x2, x3)
y <- c(y1, y2, y3)
cluster <- factor(rep(1:3, each = n_per_cluster))
data <- data.frame(x = x, y = y, cluster = cluster)
# Plot data by clusters
ggplot(data, aes(x = x, y = y, color = cluster)) +
geom_point() +
geom_smooth(method = "lm", se = FALSE) +
ggtitle("Relationships within Clusters")
# Analyze pooled data
ggplot(data, aes(x = x, y = y)) +
geom_point() +
geom_smooth(method = "lm", color = "blue", se = FALSE) +
ggtitle("Relationship in Pooled Data")
d <- data.frame(x1 = data$x, x2 = rnorm(150), y = data$y )
plot_ly(data = d, x = ~x1, y = ~y, z = ~x2)
d <- data.frame(x1 = data$x, x2 = rnorm(150, sd = .0001), y = data$y )
plot_ly(data = d, x = ~x1, y = ~y, z = ~x2)
# Apply clusterwise regression
result <- clusterwise_regression(data, k = 3)
# Apply clusterwise regression
result <- clusterwise_regression(data = d, k = 3)
# Output
print(result$clusters) # Cluster assignments
print(result$betas)    # Regression coefficients for each cluster
print(result$converged) # Convergence status
plot_ly(data = d, x = ~x1, y = ~y, z = ~x2, color = result$clusters)
install.packages("randomForest")
library(randomForest)
?randomForest
?boxplot
?randomForest::randomForest()
install.packages("lightgbm")
?lightgbm::lightgbm()
install.packages("xgboost")
?xgboost::xgboost()
?xgboost::xgb.DMatrix()
library(e1071)
?svm
# Load necessary library
library(rpart)
# Set seed for reproducibility
set.seed(123)
# Generate synthetic data
n <- 1000
x1 <- rnorm(n)
x2 <- rnorm(n)
y <- ifelse(x1 + x2 > 0, 1, 0)  # True labels based on a linear decision boundary
plot(x1,x2)
plot(x1,x2, col = y)
# Generate synthetic data
n <- 1000
x1 <- rnorm(n)
x2 <- rnorm(n)
y <- ifelse(x1 + x2 > 0, 1, 0)  # True labels based on a linear decision boundary
plot(x1,x2, col = y+1)
y_noisy <- y
y_noisy[noise_indices] <- 1 - y_noisy[noise_indices]
# Split data into training and test sets
train_indices <- sample(1:n, size = 0.7 * n)
test_indices <- setdiff(1:n, train_indices)
train_data <- data.frame(x1 = x1[train_indices], x2 = x2[train_indices], y = y_noisy[train_indices])
test_data <- data.frame(x1 = x1[test_indices], x2 = x2[test_indices], y = y[test_indices])  # Use true labels for evaluation
# Train a decision tree classifier
model <- rpart(y ~ x1 + x2, data = train_data, method = "class")
# Make predictions
pred <- predict(model, newdata = test_data, type = "class")
# Evaluate accuracy
accuracy <- mean(pred == test_data$y)
cat("Classification accuracy on the test set:", accuracy, "\n")
model <- lm(y ~ x1 + x2)
model <- glm(y ~ x1 + x2, family = 'binomial')
model <- lm(y ~ x1 + x2)
# Make predictions
pred <- predict(model, newdata = test_data, type = "class")
# Train a decision tree classifier
model <- rpart(y ~ x1 + x2, data = train_data, method = "class")
# Make predictions
pred <- predict(model, newdata = test_data, type = "class")
# Evaluate accuracy
accuracy <- mean(pred == test_data$y)
cat("Classification accuracy on the test set:", accuracy, "\n")
# Add label noise: flip 10% of the labels
noise_indices <- sample(1:n, size = 0.1 * n)
y_noisy <- y
y_noisy[noise_indices] <- 1 - y_noisy[noise_indices]
# Split data into training and test sets
train_indices <- sample(1:n, size = 0.7 * n)
test_indices <- setdiff(1:n, train_indices)
train_data <- data.frame(x1 = x1[train_indices], x2 = x2[train_indices], y = y_noisy[train_indices])
test_data <- data.frame(x1 = x1[test_indices], x2 = x2[test_indices], y = y[test_indices])  # Use true labels for evaluation
# Train a decision tree classifier
model <- rpart(y ~ x1 + x2, data = train_data, method = "class")
# Make predictions
pred <- predict(model, newdata = test_data, type = "class")
# Evaluate accuracy
accuracy <- mean(pred == test_data$y)
cat("Classification accuracy on the test set:", accuracy, "\n")
# Add label noise: flip 10% of the labels
noise_indices <- sample(1:n, size = 0.3 * n)
y_noisy <- y
y_noisy[noise_indices] <- 1 - y_noisy[noise_indices]
# Split data into training and test sets
train_indices <- sample(1:n, size = 0.7 * n)
test_indices <- setdiff(1:n, train_indices)
train_data <- data.frame(x1 = x1[train_indices], x2 = x2[train_indices], y = y_noisy[train_indices])
test_data <- data.frame(x1 = x1[test_indices], x2 = x2[test_indices], y = y[test_indices])  # Use true labels for evaluation
# Train a decision tree classifier
model <- rpart(y ~ x1 + x2, data = train_data, method = "class")
# Make predictions
pred <- predict(model, newdata = test_data, type = "class")
# Evaluate accuracy
accuracy <- mean(pred == test_data$y)
cat("Classification accuracy on the test set:", accuracy, "\n")
plot(x1,x2, col = y_noisy+1)
# Split data into training and test sets
train_indices <- sample(1:n, size = 0.7 * n)
test_indices <- setdiff(1:n, train_indices)
train_data <- data.frame(x1 = x1[train_indices], x2 = x2[train_indices], y = y_noisy[train_indices])
test_data <- data.frame(x1 = x1[test_indices], x2 = x2[test_indices], y = y[test_indices])  # Use true labels for evaluation
# Train a decision tree classifier
model <- rpart(y ~ x1 + x2, data = train_data, method = "class")
# Make predictions
pred <- predict(model, newdata = test_data, type = "class")
# Evaluate accuracy
accuracy <- mean(pred == test_data$y)
cat("Classification accuracy on the test set:", accuracy, "\n")
# Add label noise: flip 10% of the labels
noise_indices <- sample(1:n, size = 0.5 * n)
y_noisy <- y
y_noisy[noise_indices] <- 1 - y_noisy[noise_indices]
plot(x1,x2, col = y_noisy+1)
# Split data into training and test sets
train_indices <- sample(1:n, size = 0.7 * n)
test_indices <- setdiff(1:n, train_indices)
train_data <- data.frame(x1 = x1[train_indices], x2 = x2[train_indices], y = y_noisy[train_indices])
test_data <- data.frame(x1 = x1[test_indices], x2 = x2[test_indices], y = y[test_indices])  # Use true labels for evaluation
# Train a decision tree classifier
model <- rpart(y ~ x1 + x2, data = train_data, method = "class")
# Make predictions
pred <- predict(model, newdata = test_data, type = "class")
# Evaluate accuracy
accuracy <- mean(pred == test_data$y)
cat("Classification accuracy on the test set:", accuracy, "\n")
library(class)
train_indices <- sample(1:n, size = 0.7 * n)
test_indices <- setdiff(1:n, train_indices)
train_data <- data.frame(x1 = x1[train_indices], x2 = x2[train_indices], y = y_noisy[train_indices])
test_data <- data.frame(x1 = x1[test_indices], x2 = x2[test_indices], y = y[test_indices])  # Use true labels for evaluation
# Train a k-NN classifier (k = 5)
train_features <- train_data[, c("x1", "x2")]
test_features <- test_data[, c("x1", "x2")]
# Apply k-NN
k <- 5
pred <- knn(train = train_features, test = test_features, cl = train_data$y, k = k)
# Evaluate accuracy
accuracy <- mean(pred == test_data$y)
cat("Classification accuracy on the test set:", accuracy, "\n")
# Generate synthetic data
n <- 1000
x1 <- rnorm(n)
x2 <- rnorm(n)
y <- ifelse(x1 + x2 > 0, 1, 0)  # True labels based on a linear decision boundary
plot(x1,x2, col = y_noisy+1)
# Add label noise: flip 10% of the labels
noise_indices <- sample(1:n, size = 0.1 * n)
y_noisy <- y
y_noisy[noise_indices] <- 1 - y_noisy[noise_indices]
plot(x1,x2, col = y_noisy+1)
# Split data into training and test sets
train_indices <- sample(1:n, size = 0.7 * n)
test_indices <- setdiff(1:n, train_indices)
train_data <- data.frame(x1 = x1[train_indices], x2 = x2[train_indices], y = y_noisy[train_indices])
test_data <- data.frame(x1 = x1[test_indices], x2 = x2[test_indices], y = y[test_indices])  # Use true labels for evaluation
# Train a decision tree classifier
model <- rpart(y ~ x1 + x2, data = train_data, method = "class")
# Make predictions
pred <- predict(model, newdata = test_data, type = "class")
# Evaluate accuracy
accuracy <- mean(pred == test_data$y)
cat("Classification accuracy on the test set:", accuracy, "\n")
library(class)
train_indices <- sample(1:n, size = 0.7 * n)
test_indices <- setdiff(1:n, train_indices)
train_data <- data.frame(x1 = x1[train_indices], x2 = x2[train_indices], y = y_noisy[train_indices])
test_data <- data.frame(x1 = x1[test_indices], x2 = x2[test_indices], y = y[test_indices])  # Use true labels for evaluation
# Train a k-NN classifier (k = 5)
train_features <- train_data[, c("x1", "x2")]
test_features <- test_data[, c("x1", "x2")]
# Apply k-NN
k <- 5
pred <- knn(train = train_features, test = test_features, cl = train_data$y, k = k)
# Evaluate accuracy
accuracy <- mean(pred == test_data$y)
cat("Classification accuracy on the test set:", accuracy, "\n")
# Add label noise: flip 10% of the labels
noise_indices <- sample(1:n, size = 0.3 * n)
y_noisy <- y
y_noisy[noise_indices] <- 1 - y_noisy[noise_indices]
plot(x1,x2, col = y_noisy+1)
# Split data into training and test sets
train_indices <- sample(1:n, size = 0.7 * n)
test_indices <- setdiff(1:n, train_indices)
train_data <- data.frame(x1 = x1[train_indices], x2 = x2[train_indices], y = y_noisy[train_indices])
test_data <- data.frame(x1 = x1[test_indices], x2 = x2[test_indices], y = y[test_indices])  # Use true labels for evaluation
# Train a decision tree classifier
model <- rpart(y ~ x1 + x2, data = train_data, method = "class")
# Make predictions
pred <- predict(model, newdata = test_data, type = "class")
# Evaluate accuracy
accuracy <- mean(pred == test_data$y)
cat("Classification accuracy on the test set:", accuracy, "\n")
library(class)
train_indices <- sample(1:n, size = 0.7 * n)
test_indices <- setdiff(1:n, train_indices)
train_data <- data.frame(x1 = x1[train_indices], x2 = x2[train_indices], y = y_noisy[train_indices])
test_data <- data.frame(x1 = x1[test_indices], x2 = x2[test_indices], y = y[test_indices])  # Use true labels for evaluation
# Train a k-NN classifier (k = 5)
train_features <- train_data[, c("x1", "x2")]
test_features <- test_data[, c("x1", "x2")]
# Apply k-NN
k <- 5
pred <- knn(train = train_features, test = test_features, cl = train_data$y, k = k)
# Evaluate accuracy
accuracy <- mean(pred == test_data$y)
cat("Classification accuracy on the test set:", accuracy, "\n")
176889.74 * 0.0195/12
a <- 176889.74 * 0.0195/12
b <- 176889.74 * 0.018/12
a <- 176889.74 * 0.0195/12
b <- 176889.74 * 0.018/12
c <- 127083.15 * 0.0195/12
d <- 127083.15 * 0.018/12
(a-b) + (c-d)
library(rio)
data <- import('C:/Users/jeffr/Downloads/CSV_A_NL57RABO0310915554_EUR_20240707_20241215.csv')
View(data)
Which(data$'Naam tegenpartij' == 'Creditrente')
hich(data$'Naam tegenpartij' == 'Creditrente')
which(data$'Naam tegenpartij' == 'Creditrente')
id <- which(data$'Naam tegenpartij' == 'Creditrente')
data[id,]
d <- data[id,]
export(data, file = 'C:/Users/jeffr/Downloads/creditrente_bouwdepot.csv')
export(d, file = 'C:/Users/jeffr/Downloads/creditrente_bouwdepot.csv')
export(d, file = 'C:/Users/jeffr/Downloads/creditrente_bouwdepot.xlsx')
47363.41+68464.8+68464.8
library(shiny)
source("~/.active-rstudio-document", echo=TRUE)
runApp('Prive/Spinozahuis/Educatiecommissie/ShinyAppSterrekijken/Sterrenbeeld.R')
runApp('Prive/Spinozahuis/Educatiecommissie/ShinyAppSterrekijken/Sterrenbeeld.R')
runApp('Prive/Spinozahuis/Educatiecommissie/ShinyAppSterrekijken/Sterrenbeeld.R')
runApp('Prive/Spinozahuis/Educatiecommissie/ShinyAppSterrekijken/Sterrenbeeld.R')
runApp('Prive/Spinozahuis/Educatiecommissie/ShinyAppSterrekijken/Sterrenbeeld.R')
runApp('Prive/Spinozahuis/Educatiecommissie/ShinyAppSterrekijken/Sterrenbeeld.R')
getwd()
setwd('C:/Users/jeffr/OneDrive/Documents/Prive/Spinozahuis/Educatiecommissie/ShinyAppSterrekijken/')
runApp('Sterrenbeeld.R')
runApp('Sterrenbeeld.R')
img(src = file.path('www', zodiac_images$Leo))
runApp('Sterrenbeeld.R')
runApp('Sterrenbeeld.R')
runApp('Sterrenbeeld.R')
runApp('Sterrenbeeld.R')
runApp('Sterrenbeeld.R')
runApp('Sterrenbeeld.R')
runApp('Sterrenbeeld.R')
runApp('Sterrenbeeld.R')
runApp('Sterrenbeeld.R')
runApp('Sterrenbeeld.R')
runApp('Sterrenbeeld.R')
runApp('Sterrenbeeld.R')
source("~/.active-rstudio-document", echo=TRUE)
runApp('Sterrenbeeld.R')
rsconnect::deployApp(forceUpdate = TRUE)
